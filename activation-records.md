### What's News


### How Do You Stack Up?

In this issue of the Daily PL, although we will recap the concepts related to the runtime stack ... we will focus on the code a compiler generates -- we will work with the C programming language -- that manipulates the stack. We will dissect how that code actually works by really digging into the details and relating our conceptual understanding of the runtime stack with the technical details. In our description of the code generated by the compiler, you may be unfamiliar with the syntax if you have never seen machine code before. Don't worry! I will explain the purpose of each instruction in a way that (I hope!) makes it possible for you to make sense of the code.

Before going forward, be sure to page into memory (pun definitely intended) the concepts and vocabulary that we learned in our conceptual discussion of the runtime stack: _prologue_, _epilogue_, and _invariants_ (the general meaning of the term and the invariants that apply in this context). You can read Daily PL issues about those concepts [here](./stack-frames.md) and [here](./subprograms-coroutines.md).

One final note: The perspective of "we" will shift throughout our exploration. At various points, "we" will be a function calling another function. At other times, "we" will be a function that is being called. Be sure to pay close attention to the context!

To follow along (and verify that I am not lying), you can see the code the compiler actually generates at [Godbolt](https://godbolt.org/z/b1n4h1v68)

### It's Subjective
Subprograms are the lifeblood of our programs. They are a fundamental tool for designing software that is reusable and maintainable. Even if we are writing software in languages that fit the object-oriented, functional or logic paradigm, we use subprograms to package up functionality so that other programmers can reuse it. Subprograms are a form of _process abstraction_ because they _hide_ the details of how a particular task is accomplished.

In programs built on an architecture of process abstraction using subprograms, programs are (usually) always executing in the context of _some_ function. Just think about C and C++: The program's execution starts with the code in the `main` function. The code in that function can call other functions, but when the `main` function ends, the program does, too. In other words, you cannot escape the clutches of the `main` function and the execution of a program is nothing more than a series of invocations of (other) subprograms. That is a slightly facetious statement, but it's not that far from the truth, either.

Before going any further, let's get some (very important) things straight!

1. There is only one subprogram that is executing at a time.
2. There can be many subprograms that are active simultaneously, however.

We can get that distinction between active and executing subprograms by thinking about how programmers use functions. One function may need to delegate authority to another to accomplish some task. The executing function, $E$, _invokes_ another function, $A$. That invocation happens at a particular spot in the program's code -- call it $P_E$. Upon performing the invocation, $E$ pauses and $A$ starts executing. Yes, $E$ is _still_ active, but it's definitely not executing. Once $A$ is done executing, the program will begin execution at the place where $E$ invoked $A$ -- $P_E$, right? To keep that coordination going between active subprograms and the executing subprogram during subprogram invocation and subprogram completion, there are things that must be remembered.

That data that must be remembered forms part of the information that is stored in a function's activation record[^0]. But, that's not all the information stored in that incredible data structure. The activation record, an instance of which exists for each of the active functions and the executing function, also provides the memory necessary to store the values of variables with stack-dynamic lifetime. Destroying the activation record of the executing function when it completes serves a very important purpose: It "exposes" the activation record of the active function that invoked the just-completed subprogram. That makes it very easy to restart the calling function because the space for the just-completed subprogram is properly released and the space for variables in the subprogram that is about to restart is back at the top of the stack!

[^0]: This data structure is also known as a stack frame -- I will use the terms interchangeably.

### Don't Make Me Turn This Car Around

As we discussed in previous editions of the Daily PL, there is an invariant on the structure of the runtime stack that the code generated by the language's compiler maintains: That the stack frame for the executing function is always 

a. At the top of the runtime stack of all existing activation records, and it
b. Can be referenced by an easily accessible pointer.

Even though the compiler has to generate code to maintain that invariant throughout the lifetime of the program, the invariant makes it much easier for the compiler to generate code to keep track of the storage for local variables and to generate code to perform the actually "logic" of the function.

### One Plus One Equals ... Three?

Now that we are completely clear on the concepts, let's start to dive into our exploration of the code that the compiler generates in order to maintain the runtime stack. In order to make our discussion more concrete, let's write some C code. First, there will be a function that has two parameters (which are numbers), adds them together, and returns the result. Let's be creative, and say that the function is named `add`. Woah -- crazy, I know!

```C++

int add(int left, int right) {
    int result = left + right;
    return result;
}
```

Let's also say that we have a function that has a single parameter, multiplies its argument by two and returns the result:

```C++

int multiply_by_two(int a) {
    int multiplied = add(a,a);
    return multiplied;
}
```

Let's get wild and crazy and use these two functions to calculate $5 + 2 * 5$:

```C++

int main() {
    int five = 5;
    int five_times_two = multiply_by_two(five);
    int sum = add(five, five_times_two);
    return 0;
}
```

### Get Fired Up

> Note: We are going to proceed first directly to the call of `multiply_by_two` from `main` and discuss how the invocation of a function works. We will come back later to discuss how local variables are handled.

To facilitate the process abstraction wraught by subprograms, programming language designers and compiler implementers want to help their users (those who program in that language) maintain a complete firewall between code that invokes a subprogram and the code of the subprogram being called. 

That's a good enough reason to make sure that the calling function and the called function are kept completely separate. But, there's more: If neither the calling function nor the called function know anything about each other's internal details, it will force the two parties to agree on a specific plan for how to call one another. Once a standard, agreed-upon way that one function can invoke another is chosen, then that makes it _that_ much easier for a function written in one language and compiled by one compiler, to invoke a function written in a different language and compiled by a different compiler. 

Standards are really amazing.

![Standards](https://imgs.xkcd.com/comics/standards.png)

_As a caller of `multiply_by_two`_, **we** want to (metaphorically) put the value for `multiply_by_two`'s parameter (its called the _argument_, remember!) into an envelope, drop it in one of those awesome blue mailboxes and then wait patiently at our door for the result to come back! When the letter is returned, we will pick up right where we left off.

If opening, closing and mailing an envelope is the only way in which we interact with `multiply_by_two`, then there is absolutely no way to break the abstraction provided by the subprogram. Our visibility into the _how_ of `multiply_by_two` is completely obscured. What's more, `multiply_by_two` can't see us, either. We can only communicate with one another through envelopes.

And, if you know anything about the USPS, you know that mailing a letter is not exactly instantaneous. So, when our argument-laden mailing arrives at `multiply_by_two`, `multiply_by_two` better know exactly what to do with it. If it does not, then there is going to be another time-wasting roudtrip of letters sent between the two parties to hash out any ommitted details.

Although the USPS is one of the best things provided to US citizens by the government, it is not magic. An envelope that we mail must be labeled with the address of the recepient. Otherwise, there's no chance it will arrive at its destination.

Filling an envelope with the arguments, labelling the envelope and dropping it in the mailbox are all actions that **we** have to take in order to ~~mail our letter~~ call a function. In technical terms, these actions are known as the prologue: The series of steps that code generated by the compiler must take to prepare the program's execution to transfer to the called function. These are not the only steps of the prologue, however they are all parts of the prologue that are the responsibility of the code calling another subprogram. 

Why must the **we** (the caller) be the ones that perform this subset of steps defined by the compiler? Because we (again, "we" are the caller!) are the only ones who know the value we want multiplied. There's just no other way that it could work!

The code that the compiler generates for whatever functionality is in `multiply_by_two` must be generic enough in order to double _any_ number that a programmer who calls the function specifies. The writer of `multiply_by_two` cannot make any assumptions about the values that its users want to manipulate (other than the fact that the values specified are actually, you know, numbers).

Note: Think about how the type of a parameter _does_ play a role in establishing some assumptions about arguments that the code generated by the compiler for the implementation of a subprogram.

It's been long enough ... how are the steps of the prologue we just described (put the argument(s) into the envelope, address the mailer and drop it in the mailbox) translated into code?


### A Hardware Interlude (#1)

Well, not so fast. We talked above (and in previous issues of the [Daily](./stack-frames.md) [PL](./subprograms-coroutines.md)) about the runtime stack as the place where the activation records are stored. Because the space for local variables is controlled by those activation records, the memory for local variables is also in the runtime stack. We talked, as well, about how the memory on our computer is logically divided into two sections: the stack and the heap. 

> Note: What follows does not consider virtual memory. This is not an OS class.

At the beginning of your program's execution, the program's stack memory ("the stack") exists at very _high_ memory addresses and the program's heap memory ("the heap") exists at very _low_ memory addresses. As you request more and more space from the stack, it _grows down_. As you request more and more space from the heap, it _grows up_. 

Keeping track of the valid ranges of addresses that your program can access in the stack and the heap requires cooperation between the code that the compiler generates for your program and the operating system. When your program needs to increase the amount of memory available to the heap, there is coordination with the operating system required.

That is _not_ true about your program's relationship with stack memory. In fact, the start of the stack is (more or less) fixed from the time that your program starts. Given its name (_stack memory_) it should not surprise you to find out that your program's ability to modify the size of the stack is limited to its ability to manipulate its "head" (or ["top"](https://en.wikipedia.org/wiki/Stack_(abstract_data_type))). If your program needs more space on the stack, it adjusts the head of the stack. If you program needs less space (and wants to give the excess back to the computer), then it (again!) adjusts the head of the stack. 

For allocation, the head of the stack is adjusted down. Do you know why? For deallocation, the head of the stack is adjusted up!

That's great, but where is the information about the head of the stack stored. We've talked about how activation records are created and destroyed when a function is called and when a function completes, respectively. Because you are all smart software engineers, you use lots of functions in your code to make it more readable. It stands to reason, then, that your program will need to make frequent accesses and updates to the head of the stack. 

The computer hardware gives your programs an assist here: it devotes a register (full time) to the position of the head of the stack. That register is named, wait for it the _stack pointer_. 

Great! We've connected the conceptual ideas of the runtime stack with stack memory with how that stack memory is allocated, managed, and deallocated by a running program. 

Again, modifying the size of the heap requires some coordination with the operating system. That is not true of the stack. The code that the compiler generates for your program is entirely responsible for keeping track of the head of the stack and it does so by generating code that painstakingly manipulates the stack pointer.

Read on to see it in action!

### Does Anyone Lick Stamps Anymore?

Okay, *now* we are ready to see some machine code. Here is a slightly annotated version of our `main` function:

```C++
int main() {
    int five = 5;
    int five_times_two = /* HANDOFF */ multiply_by_two(five);
    int sum = add(five, five_times_two);
    return 0;
}
```
The point marked `HANDOFF` is where we make our visit to the post office[^1]. And here is the code that a C compiler generates that goes (more or less) in that spot:

[^1]: Yes, I know that I earlier said that we mailed our letter at the mailbox, but variety is the spice of life.

```asm
push    DWORD PTR [ebp-4]
call    multiply_by_two
```

I know, right? Where's the rest of the code? There is so much happening in those two lines.

First things, first: Let's put the argument into the envelope. Remember, the code that we are dissecting is emitted by the compiler with the purpose of building up a new stack frame that will be at the top of the runtime stack when the function `add` begins. For this issue of the Daily PL, the name of the register that holds the stack pointer is named `esp` and, as we discussed above, the stack pointer always points at the top of the stack. But beware: there is nothing in the hardware itself that maintains that invariant that the value of `esp` is always the address of the top of the stack -- it's one "sub invariant" of the overall invariant that the code generated by the compiler is required to maintain to keep the runtime stack healthy.

The `push` _op code_ is in cahoots with the stack pointer: a `push` _pushes_ a value onto the top of the runtime stack. It does so by implicitly accessing and then updating the value of the `esp` register. In `push DWORD PTR [ebp - 4]`, the value that the instruction pushes onto the top of the runtime stack is the _double word_[^2]-sized chunk of memory starting four bytes after the address stored in the `ebp` register (don't worry, we will come back to the `ebp` register soon).

[^2]: See section 4.1 of the [Intel Basic Architecture](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html).

Reading a description of what is happening is not always the best way to build an intuition for what is going on. So, let's look at some pictures.

![](./graphics/Activation%20Records%20-%201.jpg)

The figure above shows the state of the platform's memory (and the value of two of the CPU's registers -- `ebp` and `esp`) just _before_ the `push` instruction is executed. As you can see, `0x00000005` is already in memory. We have not yet seen the code that makes that happen, but we will soon enough. The dashed red line indicates the boundary of `main`'s activation record. For future reference, note where `ebp` is currently pointing! You can see that the `esp` is pointing at the lowest point in _stack_ memory associated with the activation record for `main`. That's really neat -- it _seems_ to demarcate the end of the runtime stack from the abyss!

![](./graphics/Activation%20Records%20-%202.jpg)

The second figure looks very similar, but there are a few very important differences. This graphic represents the state of the computer's memory after the `push` operation has been performed. Notice, first, that the value `0x00000005` is on stack twice now. Also, notice how `esp` is now _lower_ than it was before? That's because of the way that the `push` operation implicitly modifies the value of `esp` as well as the way that it stores values in memory.

Remember that we talked in class about the _direction_ of growth for the stack and the heap. We said that the heap grows _up_ while the stack grows _down_. You can see how that plays out in a running system by looking at what changed in the two diagrams above. The `push` operation that pushed a value onto the runtime stack implicitly changed the value of `esp` at the same time that it wrote a value into stack memory.

The second copy of `0x00000005` is _outside_ the red dashed line that indicates the boundary of `main`'s activation record. I wonder why that is? Could it be that the code we are deciphering is beginning to build a new activation record?

### Invariable 

And, yes, our supposition is spot on -- the code that we are seeing is building the stack frame for the invocation of `multiply_by_two`. You can see that `0x5` is outside the stack frame of the (currently executing) `main` function. But, it's not outside the stack -- the `push` operation "moved `esp`". Comparing the value on the stack with the high-level source code, it looks like what just happened was that the code emitted by the compiler just put our argument in the envelope!!

Now, on to step 2 of our USPS adventure: addressing the envelope. We will need to put both a destination address _and_ a return address on our envelop. Without the former, our letter won't get to its destination. Without the latter, the receipient's response won't be able to get back to us! 

What's so amazing about the Intel architecture is that both pieces of the envelope's addressing are done with a single instruction: the `call`. The `call multiply_by_two` tells the CPU that the _next_ instruction to execute will be the first instruction at the address `multiply_by_two`[^4].

[^4]: Note that `multiply_by_two` is a convenience -- the CPU does not actually see that name. It's a memory address but our tools are nice enough to give us a name instead!

So, what about that return address? Well, the `call` instruction also "pushes" the return address (yes, that's the technical term, too!) onto the top of the stack! How cool!

![The configuration of the program's memory and selected registers after the `call` instruction in `main` invoking `multiply_by_two`.](./graphics/Activation%20Records%20-%203.jpg)

After the `call` instruction has executed, the memory (and selected registers) have the values shown above.

At this point, it seems like _we_ (again, the caller) have gone as far as we can go. We have put the argument into the envelope, addressed the envelope, and dropped it in the mail. At this point, all we can do is wait!

### ... Wrote Me a Letter Saying ...

As we described above, the `call` instruction tells the CPU that the address of the _next_ instruction to execute is not the address of the instruction immediately following the one that just executed. Rather, the `call` instruction tells the CPU that the next instruction to execute is at a specific address somewhere else in the program. For the code here, that is the address of the first instruction of the `multiply_by_two` function.

> Note: _We_ are now the function being called -- `multiply_by_two` in this case!

The first acts of the `multiply_by_two` function (and any function, really) are very important and are still considered part of the prologue. Until we see the full picture, its first operations are a little, well, mind bending.

However, their purpose is vital. To understand just how important the upcoming instructions emitted by the compiler are to the maintenance of the runtime stack invariant, think about what will happen in the future when the code generated by the compiler for the "logic" in the `multiply_by_two` function executes. As we discussed in class, there is one set of code generated by the compiler for that function and that code executes no matter how or when that function is invoked. That code needs to be able to execute correctly no matter where its local variables and arguments happen to be in memory. In class we discussed how this is solved by the so-called _base pointer_. Making sure that the _base pointer_ addresses some known point inside the function's stack frame is one of the important aspects of the runtime stack invariant that must be maintained when the program is running.

When `main` was still in charge, we saw (in the diagram) how `ebp` (the hardware name for the base pointer) is in the right spot. However, now that `multiply_by_two` is running (and that invocation of `main` is merely _active_), `ebp` is not in the right spot! 

But, can the compiler simply emit code that will change its value so that `ebp` points somewhere in the stack frame that is being built for the invocation of `multiply_by_two`? It _could_ ... but what will happen (even further in the future) when `multiply_by_two` is done and `main` begins executing again[^5]?

[^5]: Can you remember how we will know where to restart the execution of the instance of `main` that is above the instance of `multiply_by_two` on the stack?

A conundrum. What are we going to do? I know!! Before we move `ebp`, let's store its value somewhere in the stack frame that we are building. That way we will know the value to which to restore it during the interregnum between the end of the execution of the executing function (yes, I know that sounds awkward) and the resumption of the most-recently active function. If we follow this little trick at the start of every function, then we are a long way to being able to maintain that invariant about which we are so worried!

That's great theory, but is it what the code does that the compiler emitted?

```asm
push    ebp
```

Yes, in fact it is! That machine instruction (yes, it's a _push_ again!) simply copies the value currently in `ebp` onto the top of the stack!

![](./graphics/Activation%20Records%20-%204.jpg)

There _is_ a value in the memory used to store the existing value of `ebp` but we will visualize it with an arrow because it is (essentially) a pointer (to the earlier position of `ebp`).

Now that the compiler-emitted code has properly stored away the existing value of `ebp` so that it can be retrieved later, it seems logical that the next few compiler-emitted instructions adjust the current value of `ebp` so that it points within the being-built stack frame. And, surprise, that's exactly what happens:

```asm
mov     ebp, esp
```

That machine code instruction simply `mov`es the value from `esp` to the `ebp`. And, because `esp`, you know, points to the (top of the) stack, that's where the `ebp` now points, too!

![](./graphics/Activation%20Records%20-%205.jpg)

What we are starting to see is really neat. We are slowly building up a new stack frame and making sure that our invariant(s) hold.

Is it time, yet, for the code that actually preforms the operations of `multiply_by_two` to run? Not quite.

### Give Me Some Elbow Room

Recall the source code for the `multiply_by_two` function:

```C++
int multiply_by_two(int a) {
    int multiplied = add(a,a);
    return multiplied;
}
```

Notice that there is a local variable named `multiplied`. The space for local variables, as we know, goes into the stack frame. So, it seems like the compiler will need to have generated some code (that executes before the function gets down to business) that will make space for `multiplied`[^6].

[^6]: Remember how we talked about stack dynamic variables and how their memory is allocated and deallocated by code that the compiler generates? Check out what is about to happen!

The compiler generates code that uses some machine instructions we haven't seen yet, but accomplishes something that we will be familiar with: 

```asm
sub     esp, 16
```

This machine instruction `sub`tracts $16$ from the value currently stored in `esp` and stores that newer, smaller value back into the register. If you remember that the stack grows down, can you think about what is happening? 

_YES!_ The compiler has generated code that will allocate additional space on the stack!

![](./graphics/Activation%20Records%20-%206.jpg)

> Note: `multiplied` does not need $16$ bytes of storage. So, why does the compiler make $16$ bytes of space? Because of some very peculiar rules about where `esp` must point when a function is called. Ask your operating systems professor about something called _stack pointer alignment_!

And, _now_ we are ready to let `multiply_by_two` start executing code that will actually accomplish the work it is responsible for -- namely multiplying by $2$! 

![](./graphics/Activation%20Records%20-%206a.jpg)

Inside the purple-dashed box you can see an entire stack frame! How cool is that? Notice that the stack frame contains the values of the arguments, the address of where to resume the function that called _us_, the previous value of the `ebp` that can be restored when we are done executing and space for local variables. 

A little plot of land to call our own!

### Let The Games Begin (Again)

We are nowhere near out of the woods yet. The first computation that `multiply_by_two` does to (you know) multiply by two, is call the `add` function. It calls `add` with the value of its parameter (`a`) as the argument for both `left` and `right` of `add`.

So, let it all begin again. We can go a little faster this time:

```asm
push    DWORD PTR [ebp+8]
push    DWORD PTR [ebp+8]
call    add
```

It makes sense that `push` would be employed twice and have the same operand. After all, if we look at the source code, `a` is used for both the arguments. How does `DWORD PTR [ebp+8]` mean `a`? 

First, let's talk about the way that assembler is written to access memory. The `[` and `]` delimit an expression that will dereference a pointer. Think of the `[` and `]` as being akin to `*` in C and C++. It allows you to _dereference_ a pointer. And, just like in C and C++, it is not enough to speak simply of a pointer. You _must_ speak of the type of the value at the target of that pointer. In the dialect of machine code that we are reading in this issue of the Daily PL, the `DWORD PTR` serves that role -- it tells us that the target of the pointer is a double word (see above for more information on double words). Now we know that 

```asm
push    DWORD PTR [ebp+8]
```

is an attempt to dereference a pointer to a $4$-byte-wide value (the size of a double word). So, how is it that `ebp+8` is the address of the value given by the caller for the parameter `a`?

Look at the diagram again. Notice where `ebp` is currently pointing.

![](./graphics/Activation%20Records%20-%206a.jpg)

Bingo! It's pointing right "beneath" the saved `ebp` (in pink) and the return address (in yellow). The next one above the return address is ... the value of the argument given by `main` to the invocation of `multiply_by_two`! On the platform that we are using for this example, the register `ebp` can only hold $4$ bytes of data. What's more, the return address needs only $4$ bytes of storage. $4 + 4 = 8$ bytes of space are needed to store the values denoted by the pink and yellow boxes in the diagram. Because the stack grows down, because we know where `ebp` points, and because we know the size of the return address and the stashed `ebp`, then we know that `ebp + 8` will give the address of the memory that stores the value of the argument given by the caller for the parameter `a`.

Now _that_ is cool.

### I Can See Clearly Now

What you just saw is the power of the invariant of the stack frame that we have talked so much about. The code that the compiler generated for the `multiply_by_two` function does not need to care _at all_ about the _absolute_ address of the value given by the caller for the value of the parameter `a`. It only needs to know how to get there _relative_ to `ebp`. If the code generated by the compiler did _not_ maintain the invariant of the runtime stack properly, then this type of code emitted by the compiler would be hopelessly broken! We would need, instead, a version of code specific to each invocation -- where each version of the code knew exactly where the caller put its argument into memory.

### More Turtles

> Note: *We* are now `add` being invoked from `multiply_by_two`.

After `push`ing twice and addressing the envelope (executing the `call`), the contents of the platform's memory looks like ...

![](./graphics/Activation%20Records%20-%207.jpg)

You can start to see how the pattern is repeating itself! How cool is this? Based on what you see in the stack frame outlined in purple, what code should the compiler emit next? That's right, 
1. the code that will stash the `ebp` for the function that called us so that we can be restore it when we are finished, and
2. the code that will set the `ebp` to the current value of the stack pointer.

And, that's just the code the compiler has emitted:

```asm
push    ebp
mov     ebp, esp
```

Look back up above and notice that the code the compiler emitted to perform these operations is _exactly_ the same as the code emitted to perform these operations in `multiply_by_two`. That's really neat[^7].

[^7]: Those are the type of patterns that reverse engineers look for when they are digging through programs for which they do not have access to the source code. Seeing pairs of instructions like that are giveaways that code generated for a high-level function is about to begin.

After executing the two instructions above, the contents of the memory look like:

![](./graphics/Activation%20Records%20-%208.jpg)

And, we're not quite done yet establishing the stack frame for the invocation of `add` from within `multiply_by_two`. We have yet to establish the space in the stack frame necessary to store the function's local variables. `add`, like `multiply_by_two`, has a single local variable. But, in the same way that the code emitted for `multiply_by_two` allocated $16$ bytes when only $4$ are needed, the code emitted for `add` does the same:

```asm
sub     esp, 16
```

And now, voila!, we have seen all the machine code that the compiler generates and traced it as two stack frames were established!

![](./graphics/Activation%20Records%20-%209.jpg)

### What Have We Gotten Ourselves Into?

Well, we are now in a very odd spot. I could bring in some additional metaphors to describe our current predicament, but I like our running Postal analogy so much that I don't want to cause any confusion.

From the perspective of the program (and not the perspective of any single function), _we_ are two envelopes-in-the-mail deep. There are two people waiting for return envelopes in the mail -- the `main` function (that called `multiply_by_two`) and the `multiply_by_two` function (that called `add`). 

`add` takes care of all of its work itself. It does not call any other functions. Because we are mostly concerned about tracing the code generated by the compiler that maintains the runtime stack invariant and _not_ the code that the compiler emitted to implement the program's logic, let's skip to the end of the `add` function.

```C++
int add(int left, int right) {
    int result = left + right;
    // MARK
    return result;
}
```

We will begin an analysis of the code the compiler generates from `//MARK` through the end of `add`. As you know, the `return` statement in C++ names the value that should be given back to the calling function as the result. In this case, the value of the local variable `result` contains the value that will be returned to the caller.

The code generated by the compiler that comes after `//MARK` is considered the _epilogue_ -- it is the peanut butter to the prologue's jelly. In other words, whatever the prologue did to set up a new stack frame on the runtime stack when a function was invoked, the epilogue has to undo that. Again, during this in-between time, the goal of the code emitted by the compiler is to restore the invariant: the base pointer must be repositioned within the stack frame of the function that will be restarted.

In addition to restoring `multiply_by_two`'s stack frame to its position of importance before transferring execution control back to the point where it called `add`, the return value needs to be setup for delivery to `add`, too.

When the return value is small enough to fit in (usually) fewer than 64 bits, the return value is given back to the invoking function in the `eax` register. That means there should be code that the compiler emitted to move `result` into `eax`. 

```asm
mov     eax, DWORD PTR [ebp-4]
```

Look at that! Just what we expected! Wait, really? Yes, really! How is it possible to determine that `DWORD PTR [ebp-4]` is really `result`? We have experience looking at this type of syntax from earlier. In that case, though, the address of the variable was a _positive_ offset from `ebp`. Here, however, the address of the variable is at a _negative_ offset from `ebp`. Look at the map, and see what that means? That means that it sits beneath the most-recently-stored return address and above the `esp` -- perfectly inside the stack frame of the executing function. That's just what we expected. 

Remember the instructions that manipulated `esp` in both the prologues of `add` and `multiply_by_two`? As we said, those manipulations were done to make space on the stack for local variables. The subtraction that we saw occurred when `esp` matched `ebp` and, therefore, allocated space beneath the return address (that's where `ebp` is now, after all) and above `esp`. 

All that is very strong evidence that the address inside the `[` and `]` in the `mov` instruction refers to a local variable. Great! We have the return value in `eax`. 

At this point, it's possible that we would want to restart `multiply_by_two` -- and why not? We have given it the value that it wants. The reason it called `add` was to get `result` and we just made that possible. 

What could possibly go wrong?

If we restarted the execution of `add` now, our invariant would be, well, variant! The `ebp` at the time that `add` called us is not the same as the current `ebp`. What's more, there is still a ton of information on the stack that is no longer necessary!

So, we need to do two things:

1. Free the space on the stack for the local variables of this invocation of `add` and
2. Restore the base pointer.

The good news is that we can do them in any order. What's cooler is that the Intel hardware has a single instruction that does both of these tasks at the same time:

```asm
leave
```

With that single instruction, the ... Well, I could write it out, but why don't we just read the official word. The `LEAVE` instruction

> [r]eleases the stack frame set up by an earlier ENTER instruction. The LEAVE instruction copies the frame pointer (in the EBP register) into the stack pointer register (ESP), which releases the stack space allocated to the stack frame. The old frame pointer (the frame pointer for the calling procedure that was saved by the ENTER instruction) is then popped from the stack into the EBP register, restoring the calling procedureâ€™s stack frame.

Woah! That is a mighty, _mighty_ mic drop. 

> Note: You will notice that the documentation refers to an `ENTER` operation but we have not seen an `ENTER`, have we? Well, that's okay! As long as the compiler emitted instructions that accomplish what the `ENTER` would do, then we are okay. And, of course, we saw that the compiler _did_ generate code that performs the semantically equivalent operation but was just a little more loquacious.

To make the `LEAVE` operation more clear, let's visualize the work that it does in two parts. First, let's see what happens when

> The LEAVE instruction copies the frame pointer (in the EBP register) into the stack pointer register (ESP), which releases the stack space allocated to the stack frame.

![](./graphics/Activation%20Records%20-%2010.jpg)

And now, what happens when the `LEAVE` instruction completes the second half of its operations:

> The LEAVE instruction copies the frame pointer (in the EBP register) into the stack pointer register (ESP), which releases the stack space allocated to the stack frame.

![](./graphics/Activation%20Records%20-%2010a.jpg)

Now _that_ is really cool. Remember _way_ back when -- you thought that the compiler's work to emit code to store the existing value of `ebp` on the stack frame in the prologue was a waste of time! Well, now you see why it was actually really, _really_ important. Having the older version of the stack pointer right there on, well, the stack, made it so easy for the compiler to emit code to restore that aspect of the invariant as the program's execution transitions back to the function that called it.

There's just one last problem: We have to tell the CPU that the next instruction it should execute is _not_ the one following in the order of instructions in memory. Instead, we have to direct the CPU to pick up execution back at the point where `multiply_by_two` is paused and waiting for the return mail!

In another case of what-seemed-really-odd-before-now-seems-really-brilliant, notice the "thing" that is right at the place where `esp` is pointing. That's right, it's the address that the caller stored when it invoked _us_ which was the place where the caller wanted us to restart then when we were done! Perfect!

All we need to do is verify that the compiler emitted code to tell the CPU that the address of the (next) next instruction to execute is right there at the top of the stack! And, surprise, surprise, there is an instruction that does just that:

```asm
ret
```

And that is exactly the instruction that the compiler emitted after the `leave`.

### I'll Take The Check, Please

So, where does that `LEAVE` (I couldn't resist) us? Right 

![](./graphics/Activation%20Records%20-%2011.jpg)

to be exact!

Please, _please_, do yourself a favor and look at the last figure in [More Turtles](#more-turtles). What do you see? Exactly! They are *nearly*. And, what's cooler? That the next instruction that the CPU is about to execute is the instruction right after the `call` that kicked off `add`. If `multiply_by_two` didn't know better, it would have absolutely no idea that the CPU secretly transported to the land of the person answering our letter and helped them write their response!

> Note: "We" are `add` again, and the postal worker has just handed us the letter sent back by the person to whom we mailed our letter.

The first thing that I do normally do when I get mail is to rip it open and find out if I won a contest. That's not exactly what the code does that the compiler emitted for the instruction after returning from the call. Instead, the compiler has emitted:

```asm
add esp, 8
```

Curious! What does that do? Let's look at how things change as a result of executing that instruction -- that might give us a better idea of the meaning of the operation:

![](./graphics/Activation%20Records%20-%2012.jpg)

That makes it pretty clear: The 

```asm
add esp, 8
```

essentially releases the memory allocated during the execution of the code emitted for the invocation of `multiply_by_two`'s prologue. Pretty sneaky!

I hope that you can see how the pendulum is swinging back: earlier all the operations were moving the stack pointer _down_ to allocate space on the stack for new stack frames. Now, all the operations are moving the stack pointer _up_ in order to release those earlier allocations.

And, if you scroll up far enough, you will see that there is an image that looks exactly like the one just shown. The code that the compiler emitted for the epilogue (yes, we are _still_ in the epilogue) is methodically undoing all the work done by the prologue.

> Note: Think about how the structure of the runtime stack as a stack (yes, I know it's awkward phrasing) and the position of the saved base pointer and return address on the stack make it so that the caller (again!) _has_ to be the one to release the space allocated for the arguments. When a caller invoked a function we saw that the caller was the only one who could execute instructions emitted by the compiler to allocate space to store the arguments for the parameters of the function being called because it was the only one who knew the values. However, on the way back, the _value_ of the arguments doesn't matter. In other words, the called function knows enough to be able to give back the space for the arguments _before_ returning crontrol to the caller. But, what is on the runtime stack between the stack pointer and the arguments right at the last possible moment that the called function is executing? In other words, at the precise moment that the called function would want to perform the deallocation of the space for the arguments, there is something(s?) in the way!

### Like a Kid In A Candy Store

And _now_ it's our chance to rip open the envelope and see what the post officer brought us:

```asm
        mov     DWORD PTR [ebp-4], eax
```

Remember that the `multiply_by_two` put the result into `eax`? Well, between the time that the result was put into that register and now, no one has changed the register's value. So, when we perform the `mov` operation above, the code emitted by the compiler is simply instructing the program to store the result of `add` into a place in memory -- the place at $4$ bytes below the `ebp`. 

Remember back to the description earlier where we discussed how it was possible to determine that `ebp-4` was the address of `result` in `multiply_by_two`. Notice that the code above is identical! In other words, it is clear that this operation is storing the result of `multiply_by_two` into a local variable. And, because there is only one local variable in `add`, we know that the storage for the local variable `multiplied` is $4$ bytes beneat the base pointer! 

Isn't it amazing how we are able to see and decipher _exactly_ what the compiler-emitted code is doing? There's absolutely no mystery here!

### Add Nausea

At this point, the cycle begins again. The next instructions are the ones that the compiler emitted for the epilogue of `add`. If we traced the execution of those instructions, we would see that the purple stackframe is methodically dismantled and control would return to the instruction after the place in `main` where `add` was called. I encourage you to do this analysis yourself and see where it ends up!


### It Was Always The Journey

What a tour! How amazing! In this issue of the Daily PL, we recap the concepts related to the runtime stack ... but we focused on the code a compiler generates that manipulates the stack. We dissected how that code actually works by really digging into the details and relating our conceptual understanding of the runtime stack with the technical details. I hope that you now can see how the compiler has emitted code that does _exactly_ what we expect it to, but sometimes needs lots of syllables to put it into words.